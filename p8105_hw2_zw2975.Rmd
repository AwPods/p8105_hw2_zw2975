---
title: "p8105_hw2_zw2975"
author: "Zhiyu Wei"
date: 2024-09-28
output: github_document
---

```{r setup, include = FALSE}
library(tidyverse)
library(dplyr)
```


## Problem 1

```{r import dataset 1}
# Import data
NYsubway = read_csv(file = "./NYC_Transit_Subway_Entrance_And_Exit_Data.csv")

# clean variable names
NYsubway = janitor::clean_names(NYsubway) %>%
  select(line, station_name, station_latitude, station_longitude, route1, route2, route3, route4, route5, route6, route7, route8, route9, route10, route11, entry, vending, entrance_type, ada, ada_notes) %>%
mutate(entry =if_else(entry == "YES", "TRUE",
       if_else(entry == "NO","FALSE", NA_character_))) %>%  
  mutate(vending = if_else(vending == "YES", "TRUE", 
                           if_else(vending == "NO", "FALSE", NA_character_))) %>%
  mutate(entry = as.logical(entry),
         vending = as.logical(vending))
# Now entry, vending, and ada are having logical entries
```
#### short descirption of the dataset:
The `NYsubway` dataset has variables `line`, `station_name`, `station_latitude`, `station_longitude`,`route1`, `route2`, `route3`, `route4`, `route5`, `route6`, `route7`, `route8`, `route9`, `route10`, `route11`,     `entry`, `vending`, `entrance_type`, `ada`, and `ada_notes`. 

I have cleaned the variable names to have dashed lines and all written in lower case. I cut off a few variables to make it a smaller dataset and changed the variables `vending` and `entry` to have entries `TRUE` and `FALSE` and logical. 

This dataset has `r nrow(NYsubway)` rows and `r ncol(NYsubway)` columns.

I cannot guarantee if this dataset is completed cleaned to be really tidy. But it does have a better readability and have entries that can be used for more efficient data wrangling process. 

```{r problem 1 questions}
# How many distinct stations are there? 
a = NYsubway %>%
  distinct(station_name, line) %>% # using distinct function to filter out distinct line and name entries
  count()

# How many stations are ADA compliant?
b = NYsubway %>%
  filter(ada = TRUE) %>%
  distinct(station_name, line) %>%
  count()

# What proportion of station entrances / exits without vending allow entrance?
proportion_allows_entry = NYsubway %>%
  filter(vending == FALSE) %>%  # filter our no vending
  summarise(proportion_allows_entry = mean(entry == TRUE))  # calculate the proportion

# Reformat data so that route number and route name are distinct variables. 

NYsubway <- NYsubway %>% # First need to change the columns route8 to route11 into factors to be merged together with route1 to route7.
  mutate(route8 = as.factor(route8)) %>%
  mutate(route9 = as.factor(route9)) %>%
  mutate(route10 = as.factor(route10)) %>%
  mutate(route11 = as.factor(route11)) 

#merge route1 to route11 together with the names as the route number and the values to the route served. 
NYroute <-
  pivot_longer(
    NYsubway,
    route1:route11,
    names_to = "route_number",
      values_to = "route") %>%
  filter(!is.na(route))
  
# How many distinct stations serve the A train? 
c = NYroute %>%
  filter(route == "A")  %>%
  distinct (station_name,line) %>%
  count()

# Of the stations that serve the A train, how many are ADA compliant?
A_train <- NYroute %>%
  filter(route == "A") # first store the stations into a dataset

d = A_train %>%
  filter(ada == "TRUE") %>%
  distinct(station_name,line) %>%
  count() # then count the distinct number after filter

```
#### How many distinct stations are there? 
There are `r a` distinct stations.

#### How many stations are ADA compliant?
There are `r b` stations that are ADA compliant.

#### What proportion of station entrances / exits without vending allow entrance?
`r proportion_allows_entry` of all station entrances / exits allow entrances without vending.

#### How many distinct stations serve the A train? 
There are `r c` distinct stations serve the A train. 

#### Of the stations that serve the A train, how many are ADA compliant?
There are `r d` stations that are ADA compliant out of the stations that serve the A train. 

## Problem 2
```{r import dataset 2}
library(readxl) # need this package to read excel file

# Import and clean for Mr. Trash wheel

MrTrashWheel = read_excel("./202409 Trash Wheel Collection Data.xlsx", sheet = 1) %>%
  select(-15, -16) #variables "15" and "16" doesn't make sense so I excluded them. 

MrTrashWheel = janitor::clean_names(MrTrashWheel) %>% #cleaned variable names.
  filter(!is.na(dumpster)) %>% #omit rows that do not include dumpster-specific data
mutate(sports_balls = as.integer(sports_balls)) #round the number of sports balls to integers and convert to an integer variable 

# Import and clean for professor trash wheel
Prof = read_excel("./202409 Trash Wheel Collection Data.xlsx", sheet = 2)

Prof = janitor::clean_names(Prof) %>% #cleaned variable names.
  filter(!is.na(dumpster)) #omit rows that do not include dumpster-specific data

# Import and clean for Gwynnda
Gwynnda = read_excel("./202409 Trash Wheel Collection Data.xlsx", sheet = 4) 

Gwynnda = janitor::clean_names(Gwynnda) %>% #cleaned variable names.
  filter(!is.na(dumpster)) #omit rows that do not include dumpster-specific data

# Prepare for combining both datasets
Gwynnda <- Gwynnda %>%
  mutate(sheet = "Gwynnda") 

Prof <- Prof %>%
  mutate(sheet = "prof") 
  
MrTrashWheel <- MrTrashWheel %>%
  mutate(sheet = "mr_trash_wheel") %>%
  mutate(year = as.double(year))

# Combining datasets
TrashWheel = 
  bind_rows(MrTrashWheel, Prof, Gwynnda)
```
#### Report on TrashWheel dataset
After combing the 3 datasets, the final dataset,`TrashWheel` dataset, has `r nrow(TrashWheel)` rows and `r ncol(TrashWheel)` columns.

I cleaned the names of all variables in the 3 datasets before merging so that they are all lower-cased and have dash as the space between words. 

`TrashWheel` dataset has variables `dumpster`, `month`, `year`, `date`,`weight_tons`, `volume_cubic_yards`, `plastic_bottles`, `polystyrene`, `cigarette_butts`, `glass_bottles`, `plastic_bags`, `wrappers`, `sports_balls`, `homes_powered`, and `sheet`, where the variable `sheet` indicates which dataset is the observation originally from. 

```{r problem 2 questions}
# For available data, what was the total weight of trash collected by Professor Trash Wheel? 
total_weight_trash <- TrashWheel %>%
  filter(sheet == "prof") %>%
  summarise(total_weight_trash = sum(weight_tons))

# What was the total number of cigarette butts collected by Gwynnda in June of 2022?
total_cigarette_butts <- Gwynnda %>%
  filter(sheet == "Gwynnda", year == 2022, month == "June") %>%
  summarise(total_cigarette_butts = sum(cigarette_butts))

```
#### For available data, what was the total weight of trash collected by Professor Trash Wheel? 
Total weight collected of trash collected by Professor Trash Wheel was `r total_weight_trash` tons.

#### What was the total number of cigarette butts collected by Gwynnda in June of 2022?
Total number of cigarette butts colleced by Gwynnda in June of 2022 was `r total_cigarette_butts`.

## Problem 3
```{r import dataset 3}
bakers = read_csv(file = "./gbb_datasets/bakers.csv")
bakers <- janitor::clean_names(bakers)

bakes = read_csv(file = "./gbb_datasets/bakes.csv")
bakes<-janitor::clean_names(bakes)

results = read_csv(file = "./gbb_datasets/results.csv")
# column names for this dataset are on second row. 
colnames(results)<-results[2,]
# since both the first and second rows have no valid entries, we only keep the rest 
results <- results[3:1138,]
results<-janitor::clean_names(results)
#cleaned all column names in all 3 datasets
```

#### clean datsets and merge
```{r problem 3 cleaning, warning = FALSE}
# Extrapolate the first names from the dataset bakers
bakers = bakers %>%
  mutate(baker = word(baker_name, 1))

# merge bakers to bakes
bake <- left_join(bakes, bakers,by =c("baker","series" )) 

# Now if I anti join the bake and bakers dataset, I should get an empty dataset
bake_anti_1 <- anti_join(bake, bakers)
head(bake_anti_1)

# same thing with bake and bakes
bake_anti_2 <- anti_join(bake, bakes)
head(bake_anti_2)

# change results dataset's series to double
results <- results %>%
  mutate(series = as.double(series)) %>%
  mutate(episode = as.double(episode))

# First extrapolate the non-NA entries for results
results <- results 

# merge the datasets by the column "baker"
bake <- inner_join (results, bake, by = "baker") %>%
  select(-series.x, -episode.x) %>% # only keep the episode and series numbers from the valid dataset. 
  rename(series = series.y) %>%
  rename(episode = episode.y)

# there are many repetitive rows, so the logic is to only keep rows with distinct signature_bake
bake <- bake %>%
  distinct(signature_bake, .keep_all = TRUE)

# export data
write.csv(bake, "gbb_show.csv")

```
#### summary of cleaning process
I cleaned up the names for all the datasets to have a better merging process. For the results dataset, I used the 2nd row as the column name for the dataset and only keep the entries from the 3rd row since the first 2 rows of entries do not make any sense. 

Before merging, I thought through the order of merging since the 3 datasets do not have the same numbers of rows. My final decision was to merge bakers and bakes first and then use this merged dataset to merge with the results dataset since the results dataset has the biggest size in terms of number of rows. After merging the first 2 datasets, bake and bakers, I checked on how well the datasets were merged by using `anti_join`. Both datasets created by `anti_join` function have no entries, which means no unmatched entries were found. 

One problem I encountered was that after merging the pre-merged dataset, bake, with the results dataset, the entries went up to over 4000. Then I realized I could tidy the dataset by only keeping the distinct bakes since every bakes is recorded for every bakers. 

The final dataset now has `r nrow(bake)` rows and `r ncol(bake)` columns. The dataset includes important variables such as `series`, `episode`, `baker`, and `signature bake` etc. 

```{r viewers dataset cleaning and presenting}
# Import, clean, tidy, and organize the viewership data in viewers.csv. Show the first 10 rows of this dataset. 
viewers =read_csv(file = "./gbb_datasets/viewers.csv")
viewers<-janitor::clean_names(viewers)

viewers %>% 
  head(n=10)

# What was the average viewership in Season 1? 
avg_1 <- viewers %>%
  summarise(mean_season_1 = mean(series_1, na.rm = TRUE))
# In Season 5?
avg_5 <- viewers %>%
  summarise(mean_season_5 = mean(series_5, na.rm = TRUE))
```
#### What was the average viewership in Season 1? In season 5?
For season 1, the mean viewership was `r avg_1` million and season 5 had an average of `r avg_5` millions of viewers.

