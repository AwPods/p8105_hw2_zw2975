p8105_hw2_zw2975
================
Zhiyu Wei
2024-09-28

## Problem 1

``` r
# Import data
NYsubway = read_csv(file = "./NYC_Transit_Subway_Entrance_And_Exit_Data.csv")
```

    ## Rows: 1868 Columns: 32
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (22): Division, Line, Station Name, Route1, Route2, Route3, Route4, Rout...
    ## dbl  (8): Station Latitude, Station Longitude, Route8, Route9, Route10, Rout...
    ## lgl  (2): ADA, Free Crossover
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
# clean variable names
NYsubway = janitor::clean_names(NYsubway) %>%
  select(line, station_name, station_latitude, station_longitude, route1, route2, route3, route4, route5, route6, route7, route8, route9, route10, route11, entry, vending, entrance_type, ada, ada_notes) %>%
mutate(entry =if_else(entry == "YES", "TRUE",
       if_else(entry == "NO","FALSE", NA_character_))) %>%  
  mutate(vending = if_else(vending == "YES", "TRUE", 
                           if_else(vending == "NO", "FALSE", NA_character_))) %>%
  mutate(entry = as.logical(entry),
         vending = as.logical(vending))
# Now entry, vending, and ada are having logical entries
```

#### short descirption of the dataset:

The `NYsubway` dataset has variables `line`, `station_name`,
`station_latitude`, `station_longitude`,`route1`, `route2`, `route3`,
`route4`, `route5`, `route6`, `route7`, `route8`, `route9`, `route10`,
`route11`, `entry`, `vending`, `entrance_type`, `ada`, and `ada_notes`.

I have cleaned the variable names to have dashed lines and all written
in lower case. I cut off a few variables to make it a smaller dataset
and changed the variables `vending` and `entry` to have entries `TRUE`
and `FALSE` and logical.

This dataset has 1868 rows and 20 columns.

I cannot guarantee if this dataset is completed cleaned to be really
tidy. But it does have a better readability and have entries that can be
used for more efficient data wrangling process.

``` r
# How many distinct stations are there? 
a = NYsubway %>%
  distinct(station_name, line) %>% # using distinct function to filter out distinct line and name entries
  count()

# How many stations are ADA compliant?
b = NYsubway %>%
  filter(ada = TRUE) %>%
  distinct(station_name, line) %>%
  count()

# What proportion of station entrances / exits without vending allow entrance?
proportion_allows_entry = NYsubway %>%
  filter(vending == FALSE) %>%  # filter our no vending
  summarise(proportion_allows_entry = mean(entry == TRUE))  # calculate the proportion

# Reformat data so that route number and route name are distinct variables. 

NYsubway <- NYsubway %>% # First need to change the columns route8 to route11 into factors to be merged together with route1 to route7.
  mutate(route8 = as.factor(route8)) %>%
  mutate(route9 = as.factor(route9)) %>%
  mutate(route10 = as.factor(route10)) %>%
  mutate(route11 = as.factor(route11)) 

#merge route1 to route11 together with the names as the route number and the values to the route served. 
NYroute <-
  pivot_longer(
    NYsubway,
    route1:route11,
    names_to = "route_number",
      values_to = "route") %>%
  filter(!is.na(route))
  
# How many distinct stations serve the A train? 
c = NYroute %>%
  filter(route == "A")  %>%
  distinct (station_name,line) %>%
  count()

# Of the stations that serve the A train, how many are ADA compliant?
A_train <- NYroute %>%
  filter(route == "A") # first store the stations into a dataset

d = A_train %>%
  filter(ada == "TRUE") %>%
  distinct(station_name,line) %>%
  count() # then count the distinct number after filter
```

#### How many distinct stations are there?

There are 465 distinct stations.

#### How many stations are ADA compliant?

There are 465 stations that are ADA compliant.

#### What proportion of station entrances / exits without vending allow entrance?

0.3770492 of all station entrances / exits allow entrances without
vending.

#### How many distinct stations serve the A train?

There are 60 distinct stations serve the A train.

#### Of the stations that serve the A train, how many are ADA compliant?

There are 17 stations that are ADA compliant out of the stations that
serve the A train.

## Problem 2

``` r
library(readxl) # need this package to read excel file

# Import and clean for Mr. Trash wheel

MrTrashWheel = read_excel("./202309 Trash Wheel Collection Data.xlsx", sheet = 1) %>%
  select(-15, -16) #variables "15" and "16" doesn't make sense so I excluded them. 
```

    ## New names:
    ## • `` -> `...15`
    ## • `` -> `...16`

``` r
MrTrashWheel = janitor::clean_names(MrTrashWheel) %>% #cleaned variable names.
  filter(!is.na(dumpster)) %>% #omit rows that do not include dumpster-specific data
mutate(sports_balls = as.integer(sports_balls)) #round the number of sports balls to integers and convert to an integer variable 

# Import and clean for professor trash wheel
Prof = read_excel("./202309 Trash Wheel Collection Data.xlsx", sheet = 2)

Prof = janitor::clean_names(Prof) %>% #cleaned variable names.
  filter(!is.na(dumpster)) #omit rows that do not include dumpster-specific data

# Import and clean for Gwynnda
Gwynnda = read_excel("./202309 Trash Wheel Collection Data.xlsx", sheet = 4) 

Gwynnda = janitor::clean_names(Gwynnda) %>% #cleaned variable names.
  filter(!is.na(dumpster)) #omit rows that do not include dumpster-specific data

# Prepare for combining both datasets
Gwynnda <- Gwynnda %>%
  mutate(sheet = "Gwynnda") 

Prof <- Prof %>%
  mutate(sheet = "prof") 
  
MrTrashWheel <- MrTrashWheel %>%
  mutate(sheet = "mr_trash_wheel") %>%
  mutate(year = as.double(year))

# Combining datasets
TrashWheel = 
  bind_rows(MrTrashWheel, Prof, Gwynnda)
```

#### Report on TrashWheel dataset

After combing the 3 datasets, the final dataset,`TrashWheel` dataset,
has 845 rows and 15 columns.

I cleaned the names of all variables in the 3 datasets before merging so
that they are all lower-cased and have dash as the space between words.

`TrashWheel` dataset has variables `dumpster`, `month`, `year`,
`date`,`weight_tons`, `volume_cubic_yards`, `plastic_bottles`,
`polystyrene`, `cigarette_butts`, `glass_bottles`, `plastic_bags`,
`wrappers`, `sports_balls`, `homes_powered`, and `sheet`, where the
variable `sheet` indicates which dataset is the observation originally
from.

``` r
# For available data, what was the total weight of trash collected by Professor Trash Wheel? 
total_weight_trash <- TrashWheel %>%
  filter(sheet == "prof") %>%
  summarise(total_weight_trash = sum(weight_tons))

# What was the total number of cigarette butts collected by Gwynnda in June of 2022?
total_cigarette_butts <- Gwynnda %>%
  filter(sheet == "Gwynnda", year == 2022, month == "June") %>%
  summarise(total_cigarette_butts = sum(cigarette_butts))
```

#### For available data, what was the total weight of trash collected by Professor Trash Wheel?

Total weight collected of trash collected by Professor Trash Wheel was
216.26 tons.

#### What was the total number of cigarette butts collected by Gwynnda in June of 2022?

Total number of cigarette butts colleced by Gwynnda in June of 2022 was
1.812^{4}.

## Problem 3

``` r
bakers = read_csv(file = "./gbb_datasets/bakers.csv")
```

    ## Rows: 120 Columns: 5
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (3): Baker Name, Baker Occupation, Hometown
    ## dbl (2): Series, Baker Age
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
bakers <- janitor::clean_names(bakers)

bakes = read_csv(file = "./gbb_datasets/bakes.csv")
```

    ## Rows: 548 Columns: 5
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (3): Baker, Signature Bake, Show Stopper
    ## dbl (2): Series, Episode
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
bakes<-janitor::clean_names(bakes)

results = read_csv(file = "./gbb_datasets/results.csv")
```

    ## New names:
    ## Rows: 1138 Columns: 5
    ## ── Column specification
    ## ──────────────────────────────────────────────────────── Delimiter: "," chr
    ## (5): ...1, ...2, ...3, ...4, IN = stayed in; OUT = Eliminated; STAR BAKE...
    ## ℹ Use `spec()` to retrieve the full column specification for this data. ℹ
    ## Specify the column types or set `show_col_types = FALSE` to quiet this message.
    ## • `` -> `...1`
    ## • `` -> `...2`
    ## • `` -> `...3`
    ## • `` -> `...4`

``` r
# column names for this dataset are on second row. 
colnames(results)<-results[2,]
# since both the first and second rows have no valid entries, we only keep the rest 
results <- results[3:1138,]
results<-janitor::clean_names(results)
#cleaned all column names in all 3 datasets
```

#### clean datsets and merge

``` r
# Extrapolate the first names from the dataset bakers
bakers = bakers %>%
  mutate(baker = word(baker_name, 1))

# merge bakers to bakes
bake <- left_join(bakes, bakers,by =c("baker","series" )) 

# Now if I anti join the bake and bakers dataset, I should get an empty dataset
bake_anti_1 <- anti_join(bake, bakers)
```

    ## Joining with `by = join_by(series, baker, baker_name, baker_age,
    ## baker_occupation, hometown)`

``` r
head(bake_anti_1)
```

    ## # A tibble: 6 × 9
    ##   series episode baker    signature_bake       show_stopper baker_name baker_age
    ##    <dbl>   <dbl> <chr>    <chr>                <chr>        <chr>          <dbl>
    ## 1      2       1 "\"Jo\"" Chocolate Orange Cu… Chocolate a… <NA>              NA
    ## 2      2       2 "\"Jo\"" Caramelised Onion, … Raspberry a… <NA>              NA
    ## 3      2       3 "\"Jo\"" Stromboli flavored … Unknown      <NA>              NA
    ## 4      2       4 "\"Jo\"" Lavender Biscuits    Blueberry M… <NA>              NA
    ## 5      2       5 "\"Jo\"" Salmon and Asparagu… Apple and R… <NA>              NA
    ## 6      2       6 "\"Jo\"" Rum and Raisin Bake… Limoncello … <NA>              NA
    ## # ℹ 2 more variables: baker_occupation <chr>, hometown <chr>

``` r
# same thing with bake and bakes
bake_anti_2 <- anti_join(bake, bakes)
```

    ## Joining with `by = join_by(series, episode, baker, signature_bake,
    ## show_stopper)`

``` r
head(bake_anti_2)
```

    ## # A tibble: 0 × 9
    ## # ℹ 9 variables: series <dbl>, episode <dbl>, baker <chr>,
    ## #   signature_bake <chr>, show_stopper <chr>, baker_name <chr>,
    ## #   baker_age <dbl>, baker_occupation <chr>, hometown <chr>

``` r
# change results dataset's series to double
results <- results %>%
  mutate(series = as.double(series)) %>%
  mutate(episode = as.double(episode))

# First extrapolate the non-NA entries for results
results <- results 

# merge the datasets by the column "baker"
bake <- inner_join (results, bake, by = "baker") %>%
  select(-series.x, -episode.x) %>% # only keep the episode and series numbers from the valid dataset. 
  rename(series = series.y) %>%
  rename(episode = episode.y)

# there are many repetitive rows, so the logic is to only keep rows with distinct signature_bake
bake <- bake %>%
  distinct(signature_bake, .keep_all = TRUE)

# export data
write.csv(bake, "gbb_show.csv")
```

#### summary of cleaning process

I cleaned up the names for all the datasets to have a better merging
process. For the results dataset, I used the 2nd row as the column name
for the dataset and only keep the entries from the 3rd row since the
first 2 rows of entries do not make any sense.

Before merging, I thought through the order of merging since the 3
datasets do not have the same numbers of rows. My final decision was to
merge bakers and bakes first and then use this merged dataset to merge
with the results dataset since the results dataset has the biggest size
in terms of number of rows. After merging the first 2 datasets, bake and
bakers, I checked on how well the datasets were merged by using
`anti_join`. Both datasets created by `anti_join` function have no
entries, which means no unmatched entries were found.

One problem I encountered was that after merging the pre-merged dataset,
bake, with the results dataset, the entries went up to over 4000. Then I
realized I could tidy the dataset by only keeping the distinct bakes
since every bakes is recorded for every bakers.

The final dataset now has 537 rows and 11 columns. The dataset includes
important variables such as `series`, `episode`, `baker`, and
`signature bake` etc.

``` r
# Import, clean, tidy, and organize the viewership data in viewers.csv. Show the first 10 rows of this dataset. 
viewers =read_csv(file = "./gbb_datasets/viewers.csv")
```

    ## Rows: 10 Columns: 11
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## dbl (11): Episode, Series 1, Series 2, Series 3, Series 4, Series 5, Series ...
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
viewers<-janitor::clean_names(viewers)

viewers %>% 
  head(n=10)
```

    ## # A tibble: 10 × 11
    ##    episode series_1 series_2 series_3 series_4 series_5 series_6 series_7
    ##      <dbl>    <dbl>    <dbl>    <dbl>    <dbl>    <dbl>    <dbl>    <dbl>
    ##  1       1     2.24     3.1      3.85     6.6      8.51     11.6     13.6
    ##  2       2     3        3.53     4.6      6.65     8.79     11.6     13.4
    ##  3       3     3        3.82     4.53     7.17     9.28     12.0     13.0
    ##  4       4     2.6      3.6      4.71     6.82    10.2      12.4     13.3
    ##  5       5     3.03     3.83     4.61     6.95     9.95     12.4     13.1
    ##  6       6     2.75     4.25     4.82     7.32    10.1      12       13.1
    ##  7       7    NA        4.42     5.1      7.76    10.3      12.4     13.4
    ##  8       8    NA        5.06     5.35     7.41     9.02     11.1     13.3
    ##  9       9    NA       NA        5.7      7.41    10.7      12.6     13.4
    ## 10      10    NA       NA        6.74     9.45    13.5      15.0     15.9
    ## # ℹ 3 more variables: series_8 <dbl>, series_9 <dbl>, series_10 <dbl>

``` r
# What was the average viewership in Season 1? 
avg_1 <- viewers %>%
  summarise(mean_season_1 = mean(series_1, na.rm = TRUE))
# In Season 5?
avg_5 <- viewers %>%
  summarise(mean_season_5 = mean(series_5, na.rm = TRUE))
```

#### What was the average viewership in Season 1? In season 5?

For season 1, the mean viewership was 2.77 million and season 5 had an
average of 10.0393 millions of viewers.
